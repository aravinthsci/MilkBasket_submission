# -*- coding: utf-8 -*-
"""MilkBasket Frequency patten mining.ipynb

Automatically generated by Colaboratory.

"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget http://apachemirror.wuchna.com/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz

!tar -xvf spark-2.4.3-bin-hadoop2.7.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.3-bin-hadoop2.7/"

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

df = spark.createDataFrame([{"hello": "world"} for x in range(1000)])
df.show(3)

!wget https://blume-hackathon.s3.ap-south-1.amazonaws.com/milkbasket_hackathon_data.zip

!unzip milkbasket_hackathon_data.zip

import pandas as pd
import numpy as np

df = pd.read_csv("temp_bq_data/hackathon_data.csv")

df.head()

df_sample_up = df.head(2000000)

df_sample_lower = df.tail(2000000)

frames = [df_sample_up,df_sample_lower]

data = pd.concat(frames)

data.head()

from sklearn.utils import shuffle
data = shuffle(data)

data.head()

data.to_csv("data_selection.csv")

spark_df = spark.read.csv("data_selection.csv",header=True)

spark_df.show(3)

spark_df.createOrReplaceTempView("table1")

df2 = spark.sql("select distinct collect_list(distinct subcategory_id) as item from table1 group by customer_id")

df2.createOrReplaceTempView("df3")

df4 = spark.sql("select distinct row_number() over (order by item) AS id ,item as items from df3")

df4.show(2)

from pyspark.ml.fpm import FPGrowth

fpGrowth = FPGrowth(itemsCol="items", minSupport=0.5, minConfidence=0.6)

model = fpGrowth.fit(df4)

# Display frequent itemsets.
model.freqItemsets.show(truncate = False)

# Display generated association rules.
model.associationRules.show(truncate = False)

# transform examines the input items against all the association rules and summarize the
# consequents as prediction
model.transform(df4).show(truncate = False)

# Display generated association rules.
prod_data = model.associationRules

prod_pandas =prod_data.toPandas()

prod_pandas.head()

def read_data():

    for row,value in prod_pandas.iterrows():
        data = {}
        data['antecedent'] = value[0]
        data['consequent'] = value[1]
        data['confidence'] = value[2]
        data['lift'] = value[3]
        yield data

!pip install elasticsearch

from elasticsearch import Elasticsearch
from collections import deque
from elasticsearch import helpers
import json

es = Elasticsearch("http://localhost:9200")

#delete if index already defined
es.indices.delete(index="milkbasket_prod",ignore=404)
settings={
        "mappings": {
                "docs": {
                        "properties": {
                                "antecedent": { "type":"text"},
                                "consequent": { "type":"text"},
                                "confidence": { "type":"text"},
                                "lift": { "type":"text"},
                                }
                        }
                }
        }
es.indices.create(index="milkbasket_prod", ignore=400, body=settings)
#import data to index
deque(helpers.parallel_bulk(es,read_data(),index="milkbasket_prod",doc_type="docs"), maxlen=0)
es.indices.refresh()

def search(es_object, index_name, search):
    res = es_object.search(index=index_name, body=search)
    data=(res['hits']['hits'])
    for i in data:
        print(i)

if es is not None:
    search_object = {'query': {'match': {'antecedent': '1125264'}}}
    search(es, 'milkbasket_prod', json.dumps(search_object))









